const db = require('../database/db');

console.log('====================================');
console.log('  ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿æŠ•å…¥');
console.log('====================================\n');

// ã‚µãƒ³ãƒ—ãƒ«è«–æ–‡ãƒ‡ãƒ¼ã‚¿
const samplePapers = [
  {
    title: 'Attention Is All You Need',
    authors: 'Vaswani et al.',
    year: 2017,
    pdf_path: './data/pdfs/sample_attention.pdf',
    content: `Abstract
The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.

Introduction
Recurrent neural networks, long short-term memory and gated recurrent neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation. The Transformer model architecture eschews recurrence and instead relies entirely on an attention mechanism to draw global dependencies between input and output.`,
    tags: ['NLP', 'Transformer', 'Attention', 'DeepLearning']
  },
  {
    title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding',
    authors: 'Devlin et al.',
    year: 2018,
    pdf_path: './data/pdfs/sample_bert.pdf',
    content: `Abstract
We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.

Introduction
Language model pre-training has been shown to be effective for improving many natural language processing tasks. These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering.`,
    tags: ['NLP', 'BERT', 'Transformer', 'PreTraining']
  },
  {
    title: 'Deep Residual Learning for Image Recognition',
    authors: 'He et al.',
    year: 2015,
    pdf_path: './data/pdfs/sample_resnet.pdf',
    content: `Abstract
Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions.

Introduction
Deep convolutional neural networks have led to a series of breakthroughs for image classification. Deep networks naturally integrate low/mid/high-level features and classifiers in an end-to-end multi-layer fashion, and the "levels" of features can be enriched by the number of stacked layers (depth).`,
    tags: ['ComputerVision', 'ResNet', 'CNN', 'DeepLearning']
  }
];

// ã‚µãƒ³ãƒ—ãƒ«ãƒ¡ãƒ¢
const sampleMemos = [
  {
    paperId: 1,
    content: 'Transformerã®æ§‹é€ ã«ã¤ã„ã¦\n\nSelf-Attentionãƒ¡ã‚«ãƒ‹ã‚ºãƒ ãŒé©æ–°çš„ã€‚Position Encodingã§ä½ç½®æƒ…å ±ã‚’åŸ‹ã‚è¾¼ã‚€å·¥å¤«ãŒé¢ç™½ã„ã€‚'
  },
  {
    paperId: 1,
    content: 'Multi-Head Attentionã®å®Ÿè£…ãƒ¡ãƒ¢\n\nè¤‡æ•°ã®Attention Headã‚’ä¸¦åˆ—ã«è¨ˆç®—ã™ã‚‹ã“ã¨ã§ã€ç•°ãªã‚‹è¡¨ç¾éƒ¨åˆ†ç©ºé–“ã‚’å­¦ç¿’ã§ãã‚‹ã€‚'
  },
  {
    paperId: 2,
    content: 'BERTã®äº‹å‰å­¦ç¿’ã‚¿ã‚¹ã‚¯\n\n1. Masked Language Model (MLM)\n2. Next Sentence Prediction (NSP)\n\nMLMãŒç‰¹ã«åŠ¹æœçš„ã€‚'
  },
  {
    paperId: 3,
    content: 'ResNetã®Skip Connection\n\næ’ç­‰å†™åƒã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã§å‹¾é…æ¶ˆå¤±å•é¡Œã‚’è§£æ±ºã€‚æ·±ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å­¦ç¿’ã‚’å¯èƒ½ã«ã—ãŸã€‚'
  }
];

async function insertSampleData() {
  try {
    console.log('ğŸ“ è«–æ–‡ãƒ‡ãƒ¼ã‚¿ã‚’æŠ•å…¥ä¸­...\n');
    
    // è«–æ–‡ã‚’ç™»éŒ²
    for (let i = 0; i < samplePapers.length; i++) {
      const paper = samplePapers[i];
      const result = await db.papers.create(paper);
      console.log(`âœ… [${i + 1}/${samplePapers.length}] ${paper.title}`);
      
      // å¯¾å¿œã™ã‚‹ãƒ¡ãƒ¢ã‚’ç™»éŒ²
      const memos = sampleMemos.filter(m => m.paperId === i + 1);
      for (const memo of memos) {
        await db.memos.create(result.id, memo.content);
      }
      
      if (memos.length > 0) {
        console.log(`   â””â”€ ãƒ¡ãƒ¢ ${memos.length}ä»¶ã‚’è¿½åŠ `);
      }
    }
    
    console.log('\n====================================');
    console.log('âœ¨ ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®æŠ•å…¥ãŒå®Œäº†ã—ã¾ã—ãŸ!');
    console.log('====================================\n');
    console.log('æŠ•å…¥å†…å®¹:');
    console.log(`- è«–æ–‡: ${samplePapers.length}ä»¶`);
    console.log(`- ãƒ¡ãƒ¢: ${sampleMemos.length}ä»¶`);
    console.log('\nã‚¢ãƒ—ãƒªã‚’èµ·å‹•ã—ã¦ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n');
    
    process.exit(0);
  } catch (error) {
    console.error('âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:', error);
    process.exit(1);
  }
}

// ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–å¾Œã«ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿æŠ•å…¥
db.initDB();
setTimeout(() => {
  insertSampleData();
}, 1000);